use role accountadmin;

CREATE ROLE Data_Science;
CREATE ROLE developer;

GRANT USAGE ON WAREHOUSE compute_wh to data_science;
GRANT USAGE ON WAREHOUSE compute_wh to developer;
GRANT MONITOR on warehouse COMPUTE_WH to role DEVELOPER;
GRANT MONITOR on warehouse COMPUTE_WH to role data_science;

CREATE USER ds_1 PASSWORD = '****' LOGIN_NAME = ds_1 DEFAULT_ROLE = data_science
DEFAULT_WAREHOUSE = 'compute_wh' MUST_CHANGE_PASSWORD = false;

CREATE USER dev_1 PASSWORD = '****' LOGIN_NAME = dev_1 DEFAULT_ROLE = developer
DEFAULT_WAREHOUSE = 'compute_wh' MUST_CHANGE_PASSWORD = false;

GRANT ROLE data_science to USER ds_1;
GRANT ROLE developer to USER dev_1;
GRANT USAGE ON schema PUBLIC to role developer;

CREATE OR REPLACE FILE FORMAT pipe_file_format
COMPRESSION = 'AUTO' 
FIELD_DELIMITER = '|' 
RECORD_DELIMITER = '\n' 
SKIP_HEADER = 1 
FIELD_OPTIONALLY_ENCLOSED_BY = 'NONE' 
TRIM_SPACE = FALSE 
ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE 
ESCAPE = 'NONE' 
ESCAPE_UNENCLOSED_FIELD = '\134' 
DATE_FORMAT = 'AUTO' 
TIMESTAMP_FORMAT = 'AUTO' 
NULL_IF = ('\\N');

CREATE OR REPLACE STAGE stage_internal FILE_FORMAT= (FORMAT_NAME=pipe_file_format);

put file://./dataFeb-5-2020.csv @stage_internal;

ls @stage_internal;

CREATE or replace TABLE 
customer (
            customer_id STRING,
            customer_name STRING,
            customer_email STRING,
            customer_city STRING,
            customer_dob DATE
);

GRANT SELECT ON customer TO ROLE public;

COPY INTO customer (customer_id,
                customer_name,
                customer_email,
                customer_city,
                customer_dob)
FROM ( SELECT t.$1, t.$2, t.$3, t.$4, to_Date(t.$5) FROM @stage_internal t);

SELECT * FROM customer;

GRANT SELECT ON customer TO ROLE data_science;
GRANT SELECT ON customer TO ROLE DEVELOPER;

-------------------------------------------------------------
CLUSTERING

CREATE OR REPLACE WAREHOUSE performance_test warehouse_size = LARGE;

CREATE OR REPLACE DATABASE performance_db;

CREATE OR REPLACE SCHEMA perf;

use warehouse performance_test;

USE database performance_db;

USE schema perf;

CREATE OR REPLACE TABLE transactions 
(transaction_dt DATE, transaction_id INTEGER, customer_id INTEGER, amount INTEGER);

CREATE OR REPLACE STAGE s3_perf_stage URL = 's3://snowflake-essentials/streaming_data_ingest/Transactions';

COPY INTO transactions FROM @s3_perf_stage file_format = (type=csv field_delimiter='|' skip_header=1);

CREATE OR REPLACE table transactions_large (transaction_dt, transaction_id, customer_id, amount )
as select a.transaction_dt + mod(random(), 2000), random(), a.customer_id, a.amount
from transactions a cross join transactions b cross join transactions c cross join transactions d;

select count(*) from transactions_large where transaction_dt = date '2018-12-18';

select count(*) from transactions_large where transaction_dt = date '2020-03-03';

CREATE OR REPLACE table transactions_clustered_large (transaction_dt, transaction_id, customer_id, amount )
cluster by (transaction_dt)
as select a.transaction_dt + mod(random(), 2000), random(), a.customer_id, a.amount
from transactions a cross join transactions b cross join transactions c cross join transactions d;

select count(*) from transactions_clustered_large where transaction_dt = date '2018-12-18';

select count(*) from transactions_large where transaction_dt between date '2018-12-01' and date '2018-12-31';

select count(*) from transactions_clustered_large where transaction_dt between date '2018-12-01' and date '2018-12-31';

CREATE OR REPLACE table transactions_clustered_xprs (transaction_dt DATE, transaction_id INT, customer_id INT, amount INT)
cluster by ( date_trunc('MONTH', transaction_dt))
as select transaction_dt, transaction_id, customer_id, amount
from transactions_clustered_large;

select count(*) from transactions_clustered_xprs where date_trunc('MONTH', transaction_dt) = date '2018-12-01';

select count(*) from transactions_large where date_trunc('MONTH', transaction_dt) = date '2018-12-01';

select count(*) from transactions_clustered_large where date_trunc('MONTH', transaction_dt) = date '2018-12-01';
 
DROP SCHEMA perf;

DROP DATABASE performance_db;

DROP warehouse performance_test;

--------------------------------------------------
Time travel

alter session set timezone = 'UTC';

create or replace database timetraveldb;

use database timetraveldb;

create or replace schema timetravel;

use schema timetravel;

create table customer 
( name STRING, email STRING, job STRING, phone STRING, age NUMBER);

create or replace file format csv_file_format type = csv field_delimiter='|' skip_header=1;

create or replace stage s3_stage URL='s3://snowflake-essentials/' file_format =  (format_name = csv_file_format);

copy into customer from @s3_stage/sample_data_for_cloned_timetravel.csv ;

select * from customer;

select current_timestamp;
-- z

update customer set job = 'My Job';
--stmtid = abc

select * from customer at(timestamp => z::timestamp);

select * from customer before(timestamp => z+1::timestamp);

select * from customer at(offset => -60*30);

select * from customer before(statement => 'abc');

create table restored_customer clone customer before(timestamp => z+1::timestamp); 

show tables history like '%CUSTOMER%' in TIMETRAVELDB.TIMETRAVEL;
-- assume customer table had been dropped earlier. The above command will show two records for customer, with the dropped_on column for one of the records having the 
-- date when the table was dropped.

alter table customer rename to customer_latest;

undrop table customer;

DROP database timetraveldb;

DROP schema timetravel;

-----------------------------------------------------------------------------
-- Setting up access between S3 and snowflake for external storage

1. Create s3 bucket
2. Configure access permissions for the s3 bucket
        
    s3:GetObject

    s3:GetObjectVersion

    s3:ListBucket

3. create an AWS IAM policy

        {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Action": [
                      "s3:PutObject",
                      "s3:GetObject",
                      "s3:GetObjectVersion",
                      "s3:DeleteObject",
                      "s3:DeleteObjectVersion"
                    ],
                    "Resource": "arn:aws:s3:::<bucket>/<prefix>/*"
                },
                {
                    "Effect": "Allow",
                    "Action": "s3:ListBucket",
                    "Resource": "arn:aws:s3:::<bucket>",
                    "Condition": {
                        "StringLike": {
                            "s3:prefix": [
                                "<prefix>/*"
                            ]
                        }
                    }
                }
            ]
        }

4. Create an IAM role

    <refer to https://docs.snowflake.net/manuals/user-guide/data-load-s3-config.html#step-2-create-the-aws-iam-role>

5. Create a Cloud Storage Integration in Snowflake

    CREATE STORAGE INTEGRATION s3_integration
      TYPE = EXTERNAL_STAGE
      STORAGE_PROVIDER = S3
      ENABLED = TRUE
      STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::710178475289:role/mysnowflakerole'
      STORAGE_ALLOWED_LOCATIONS = ('s3://snowflake-tryout/data');
     

6. Retrieve STORAGE_AWS_IAM_USER_ARN & STORAGE_AWS_EXTERNAL_ID from the storage integration
    DESC integration s3_integration;

7. Grant the IAM User Permissions to Access Bucket Objects
    Edit the trust relationship in the IAM role created with the STORAGE_AWS_IAM_USER_ARN & STORAGE_AWS_EXTERNAL_ID
    
8. Create an external stage

    CREATE OR REPLACE DATABASE integration_db;
    
    USE DATABASE integration_db;
    
    CREATE OR REPLACE SCHEMA int;
    
    GRANT USAGE ON DATABASE integration_db TO ROLE developer;
    
    GRANT USAGE ON SCHEMA int TO ROLE developer;
    
    GRANT CREATE STAGE on schema int TO ROLE developer;
    
    GRANT CREATE FILE FORMAT ON schema int TO ROLE developer;
    
    GRANT CREATE TABLE ON schema int TO ROLE developer;
    
    USE ROLE ACCOUNTADMIN;
    GRANT USAGE ON integration s3_integration TO ROLE developer;
    
    GRANT CREATE PIPE ON schema int TO ROLE developer;
    
    GRANT CREATE STREAM ON schema int TO ROLE developer;

    USE ROLE developer;

------------------------------------------------
-- CREATING pipe

    -- login as dev_1
    
    USE warehouse compute_wh;
    
    USE database integration_db;
    
    USE schema int;
    
    CREATE OR REPLACE FILE FORMAT json_file_format type=json ;
    
    CREATE OR REPLACE STAGE s3_stage 
        STORAGE_INTEGRATION = s3_integration 
        URL = 's3://snowflake-tryout/data' 
        file_format=json_file_format;
    
    CREATE TABLE contacts_raw ( id NUMBER, name STRING, phone STRING, email STRING);

    CREATE OR REPLACE PIPE int_pipe auto_ingest = true AS
        INSERT INTO contacts_raw 
            SELECT id, name, phone, email FROM
            (
            SELECT s.$1:id::number id, 
                   s.$1:name:last::string||', '||s.$1:name:first::string name, 
                   c.value:type::string type,
                   c.value:content::string content
                FROM @s3_stage s, 
                 lateral flatten(input => s.$1, path => 'contact') m,
                 lateral flatten (input => m.value:business) c)
            PIVOT (max(content) for type IN ('phone','email')) as p(id,name, phone, email);

---At this stage, place some file(s) in the S3 bucket and query the SYSTEM$PIPE_STATUS table:

SELECT SYSTEM$PIPE_STATUS('integration_db.int.int_pipe');

---if the status is "", then we are good.
---"executionState":"RUNNING"

---Query the raw stage table to check the contents of the S3 files loaded.
SELECT * FROM contacts_raw;

--------------------------------------------------------

--Stream creation

-- standard stream creation 
    CREATE OR REPLACE STREAM contacts_changes_stream ON TABLE contacts_raw; 

    -- Target Table
    CREATE TABLE contacts ( id NUMBER, name STRING, phone STRING, email STRING);   
    
    USE ROLE ACCOUNTADMIN;
    DROP DATABASE integration_db;

-------------------------------------------------------------------
